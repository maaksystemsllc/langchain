{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LMStudio\n",
    "\n",
    ">LMStudio is an application by Meta that allows hosting of local LLMs and embedding models.\n",
    "\n",
    "This package should be used to generate embeddings. To do that, one should select an embedding model in the Inference Server tab of LMStudio.\n",
    "\n",
    "The following will show how to use LMStudioEmbeddings as a local embedding endpoint.\n",
    "\n",
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings.lmstudio import LMStudioEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Ensure that your LMStudio server is running locally and accessible at the specified URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure LMStudio server is running and accessible at http://localhost:1234"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate LMStudioEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = LMStudioEmbeddings(\n",
    "    base_url=\"http://localhost:1234\",\n",
    "    api_key=\"lm-studio\",\n",
    "    model=\"lmstudio-community/Meta-Llama-3-70B-Embedding-GGUF\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage\n",
    "\n",
    "Here's an example of how to use the instantiated `LMStudioEmbeddings` to generate embeddings and store them using FAISS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "somedocument_text = \"\"\"Your document text goes here.\"\"\"\n",
    "split_texts = text_splitter.split_text(somedocument_text)\n",
    "convstore = FAISS.from_texts(split_texts, embedding=embedding_model)\n",
	"convstore.save_local('myvectorstore.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
