{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token usage tracking\n",
    "\n",
    "LangChain offers you to track the number of consumed tokens, measure the latency of the LLM responses, and even estimate the usage cost of your application via the highly customizable `token_usage` module. You can analyze the token usage of your application directly from the code, or send the usage data to a metrics repository such as Amazon CloudWatch.\n",
    "\n",
    "The `token_usage` module splits the task of collecting the usage metrics from the LLMs via standard [Callbacks](..) in the `handlers` submodule, and the processing of the collected metrics in the `reporters` submodule.\n",
    "\n",
    "Some LLM APIs (for example, OpenAI) sends you back the number of consumed tokens after each call. Other APIs do not provide such usage information. In this case you can configure a `LocalTokenUsageCallbackHandler` instance to track the token usage directly on client side (your application's code)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Track the usage of OpenAI LLM\n",
    "\n",
    "This example shows you how to track the token usage of your LLM model directly in your application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Italy is Rome.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Local stats report:\n",
       "  total_tokens=22\n",
       "  prompt_tokens=15\n",
       "  completion_tokens=7\n",
       "  successful_requests=1\n",
       "  total_cost=0.00003650"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.callbacks.token_usage.handlers import OpenAITokenUsageCallbackHandler\n",
    "from langchain.callbacks.token_usage.reporters import LocalStatsReporter\n",
    "\n",
    "reporter = LocalStatsReporter()\n",
    "handler = OpenAITokenUsageCallbackHandler(reporter)\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", callbacks=[handler])\n",
    "prompt = PromptTemplate.from_template(\"Which city is the capital of {country}?\")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "print(chain.run(country=\"Italy\"))\n",
    "\n",
    "reporter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send the token usage to Amazon CloudWatch Metrics\n",
    "\n",
    "[Amazon CloudWatch Metrics](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/working_with_metrics.html) is a metrics database that lets you store, aggregate and analyze the performance and telemetric data of your system, including your LLM usage. \n",
    "\n",
    "To successfully execute the following cell, you should [create an AWS account](https://docs.aws.amazon.com/accounts/latest/reference/manage-acct-creating.html), [install boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/quickstart.html) and [configure your AWS credentials for boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html). \n",
    "\n",
    "You can configure any CloudWatch Metrics namespace and dimensions for your application. Some additional dimensions, like the model name and the caller id, will be injected to the dimensions list. For more information about namespace and dimensions, refer to [Amazon CloudWatch concepts docs](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_concepts.html).\n",
    "\n",
    "After successfully running the following cell, you should be able to see the recorded metrics in the [CloudWatch Metrics console](https://console.aws.amazon.com/cloudwatch/home)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital city of Italy is Rome.\n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks.token_usage.reporters import CloudWatchTokenUsageReporter\n",
    "\n",
    "reporter = CloudWatchTokenUsageReporter(\"openai_token_usage\", {\"project\": \"toke_usage_test\"})\n",
    "handler = handler = OpenAITokenUsageCallbackHandler(reporter)\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", callbacks=[handler])\n",
    "prompt = PromptTemplate.from_template(\"Which city is the capital of {country}?\")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "print(chain.run(country=\"Italy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Track usage of LLMs that do not return token consumption\n",
    "\n",
    "This cell shows you how to configure token tracking for LLMs that do not provide token usage info via their API. As an example, we will track the token usage of the Anthropic Claude-v2 model in [Amazon Bedrock service](https://aws.amazon.com/bedrock/).\n",
    "\n",
    "You can easily customize `LocalTokenUsageCallbackHandler` to any model providing a token counter and an optional cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install anthropic boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Rome is the capital and largest city of Italy.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Local stats report:\n",
       "  total_tokens=18\n",
       "  prompt_tokens=8\n",
       "  completion_tokens=10\n",
       "  successful_requests=1\n",
       "  total_cost=0.00041496"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "from langchain.llms import Bedrock\n",
    "from langchain.utilities.anthropic import get_num_tokens_anthropic\n",
    "from langchain.callbacks.token_usage.handlers import LocalTokenUsageCallbackHandler\n",
    "\n",
    "MODEL_NAME = \"anthropic.claude-v2\"\n",
    "\n",
    "def bedrock_anthropic_claude_cost_func(num_input_tokens: int, num_output_tokens: int) -> float:\n",
    "    # See https://aws.amazon.com/bedrock/pricing/\n",
    "    return num_input_tokens * 0.01102 / 1000.0 + num_output_tokens * 0.03268 / 1000.0\n",
    "\n",
    "reporter = LocalStatsReporter()\n",
    "\n",
    "handler = LocalTokenUsageCallbackHandler(\n",
    "    reporter=reporter,\n",
    "    model_name=MODEL_NAME,\n",
    "    caller_id=\"test_user\",\n",
    "    token_counter_func=get_num_tokens_anthropic,\n",
    "    cost_func=bedrock_anthropic_claude_cost_func,\n",
    ")\n",
    "\n",
    "session = boto3.Session(region_name=\"us-east-1\")\n",
    "llm = Bedrock(\n",
    "    client=session.client(\"bedrock-runtime\"),\n",
    "    model_id=MODEL_NAME,\n",
    "    model_kwargs={\n",
    "        \"max_tokens_to_sample\": 4096,\n",
    "        \"temperature\": 0.2,\n",
    "    },\n",
    "    callbacks=[handler],\n",
    ")\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"Which city is the capital of {country}?\")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "print(chain.run(country=\"Italy\"))\n",
    "\n",
    "reporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
