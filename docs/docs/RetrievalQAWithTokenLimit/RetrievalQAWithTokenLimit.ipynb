{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Required Packages to install"
      ],
      "metadata": {
        "id": "KRJwfIegzx9I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgtRJvItwwme"
      },
      "outputs": [],
      "source": [
        "!pip install langchain chromadb python-dotenv tiktoken openai scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing Libraries"
      ],
      "metadata": {
        "id": "Qx4v_u5Fz0XU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osOYSukCZD1U"
      },
      "outputs": [],
      "source": [
        "import os, tiktoken, asyncio\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "from langchain_community.document_loaders import SeleniumURLLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import RetrievalQA, RetrievalQAWithSourcesChain\n",
        "from langchain.schema import Document\n",
        "from typing import Any, Dict, List\n",
        "from langchain_core.callbacks import (\n",
        "    AsyncCallbackManagerForChainRun,\n",
        "    CallbackManagerForChainRun,\n",
        ")\n",
        "from langchain_core.documents import Document\n",
        "from typing import Tuple\n",
        "from langchain_core.pydantic_v1 import Field\n",
        "from langchain_core.retrievers import BaseRetriever\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    AIMessagePromptTemplate\n",
        ")\n",
        "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
        "from langchain.chains.qa_with_sources.base import BaseQAWithSourcesChain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "load_dotenv(find_dotenv(), override=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIAxT-YYMoLO",
        "outputId": "9b5d2c0a-3c16-4eb8-d04b-57616d1e804a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbWZEL1OGTsx"
      },
      "outputs": [],
      "source": [
        "web_links = [\"https://en.wikipedia.org/wiki/FIFA_World_Cup\"]\n",
        "loader = WebBaseLoader(web_links)\n",
        "documents = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTEWyOB-GVYa"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
        "all_splits = text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eeeIOMchGXVR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "260d04e1-7947-46f8-d059-ce338add3a7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.0.9 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ],
      "source": [
        "embeddings = OpenAIEmbeddings()\n",
        "vectorstore = Chroma.from_documents(all_splits, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = all_splits\n",
        "sys_prompt= \"\"\"\n",
        "Answer the question based on the context. If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
        "{context}\n",
        "Respond in the persona of %s\n",
        "Question: {question}\n",
        "Helpful Answer:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "9fPEG7SULi-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RetrievalQAWithTokenLimit(RetrievalQA):\n",
        "    reduce_k_below_max_tokens: bool = True\n",
        "    max_tokens_limit: int = 600\n",
        "\n",
        "    def _reduce_tokens_below_limit(self, docs: List[Document]) -> List[Document]:\n",
        "        num_docs = len(docs)\n",
        "\n",
        "        if self.reduce_k_below_max_tokens and isinstance(\n",
        "            self.combine_documents_chain, StuffDocumentsChain\n",
        "        ):\n",
        "            tokens = [\n",
        "                self.combine_documents_chain.llm_chain._get_num_tokens(doc.page_content)\n",
        "                for doc in docs\n",
        "            ]\n",
        "            token_count = sum(tokens[:num_docs])\n",
        "            while token_count > self.max_tokens_limit:\n",
        "                num_docs -= 1\n",
        "                token_count -= tokens[num_docs]\n",
        "\n",
        "        return docs[:num_docs]\n",
        "\n",
        "    def _get_docs(\n",
        "        self, question: str, *, run_manager: CallbackManagerForChainRun\n",
        "    ) -> List[Document]:\n",
        "        docs = self.retriever.get_relevant_documents(\n",
        "            question, callbacks=run_manager.get_child()\n",
        "        )\n",
        "        return self._reduce_tokens_below_limit(docs)\n",
        "\n",
        "    async def _aget_docs(\n",
        "        self, question: str, *, run_manager: AsyncCallbackManagerForChainRun\n",
        "    ) -> List[Document]:\n",
        "        docs = await self.retriever.aget_relevant_documents(\n",
        "            question, callbacks=run_manager.get_child()\n",
        "        )\n",
        "        return self._reduce_tokens_below_limit(docs)\n",
        "\n",
        "    @property\n",
        "    def _chain_type(self) -> str:\n",
        "        \"\"\"Return the chain type.\"\"\"\n",
        "        return \"retrieval_qa_with_token_limit\"\n",
        "\n"
      ],
      "metadata": {
        "id": "Do3pJQmMwgUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ae83265a"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0.2)\n",
        "retriever = vectorstore.as_retriever(search_type='similarity', search_kwargs={'k': 10})\n",
        "\n",
        "def qa(question):\n",
        "    qa_message = [\n",
        "        SystemMessagePromptTemplate.from_template(sys_prompt % (\"AI Assistant\"), input_variables=[\"context\", \"question\"]),\n",
        "    ]\n",
        "    qa_prompt = ChatPromptTemplate.from_messages(qa_message)\n",
        "    chain = RetrievalQAWithTokenLimit.from_chain_type(\n",
        "        llm = llm,\n",
        "        chain_type = \"stuff\",\n",
        "        retriever=retriever,\n",
        "        chain_type_kwargs={\"prompt\": qa_prompt, \"verbose\": True},\n",
        "    )\n",
        "    answer = chain.run(question)\n",
        "    return answer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = 'Where is the world cup?'\n",
        "answer = qa(question)\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evmLP2MpzwY-",
        "outputId": "2e299c96-59f6-47c8-f387-cfe0b26decd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: \n",
            "Answer the question based on the context. If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
            "The FIFA World Cup, often simply called the World Cup, is an international association football competition between the senior men's national teams of the members of the Fédération Internationale de Football Association (FIFA), the sport's global governing body. The tournament has been held every four years since the inaugural tournament in 1930, with the exception of 1942 and 1946 due to the Second World War. The reigning champions are Argentina, who won their third title at the 2022 tournament.\n",
            "The contest starts with the qualification phase, which takes place over the preceding three years to determine which teams qualify for the tournament phase. In the tournament phase, 32 teams compete for the title at venues within the host nation(s) over the course of about a month. The host nation(s) automatically qualify for the group stage of the tournament. The competition is scheduled to expand to 48 teams, starting with the 2026 tournament.\n",
            "\n",
            "Seventeen countries have hosted the World Cup, most recently Qatar, who hosted the 2022 event. The 2026 tournament will be jointly hosted by Canada, the United States and Mexico, which will give Mexico the distinction of being the first country to host games in three World Cups.\n",
            "\n",
            "As of the 2022 FIFA World Cup, 22 final tournaments have been held since the event's inception in 1930, and a total of 80 national teams have competed. The trophy has been won by eight national teams. Brazil, with five wins, are the only team to have played in every tournament. The other World Cup winners are Germany and Italy, with four titles each; Argentina, with three titles; France and inaugural winner Uruguay, each with two titles; and England and Spain, with one title each.\n",
            "The World Cup is the most prestigious association football tournament in the world, as well as the most widely viewed and followed single sporting event in the world.[1][2] The viewership of the 2018 World Cup was estimated to be 3.57 billion, close to half of the global population,[3][4] while the engagement with the 2022 World Cup was estimated to be 5 billion, with about 1.5 billion people watching the final match.[5]\n",
            "\n",
            "See also: World championships\n",
            "Respond in the persona of AI Assistant\n",
            "Question: Where is the world cup?\n",
            "Helpful Answer:\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "The FIFA World Cup is held in various host nations around the world. The most recent tournament was hosted in Qatar in 2022. The upcoming 2026 tournament will be jointly hosted by Canada, the United States, and Mexico.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}