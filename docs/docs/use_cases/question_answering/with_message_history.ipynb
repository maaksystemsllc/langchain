{
 "cells": [
  {
   "cell_type": "raw",
   "id": "b98ca562",
   "metadata": {},
   "source": [
    "---\n",
    "sidebar_position: 10\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db997dff",
   "metadata": {
    "id": "0a254ed4"
   },
   "source": [
    "# With MessageHistory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a153f2b0",
   "metadata": {
    "id": "fbb72b56"
   },
   "source": [
    "In the previous content, we introduced how to use RAG to enhance the capabilities of the chat question and answer system.However, if we want to introduce RAG into a conversation with chat memory, so that we can enhance the customization capabilities in a conversation with chat history. In this tutorial, we will be based on the original chat record saved in Sqlite, allowing users to pass in the previous conversation [blog of lilianweng](https://lilianweng.github.io/posts/2023-06-23-agent/) data to expand RAG in chat conversations.About creating a simple conversation chain which uses ConversationEntityMemory backed by a SqliteEntityStore,please see this [doc](https://python.langchain.com/docs/integrations/memory/sqlite)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3016234d",
   "metadata": {
    "id": "1b79e462"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87831748",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-02T03:18:30.908881Z",
     "start_time": "2024-03-02T03:18:30.894154Z"
    },
    "id": "8394ead4"
   },
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8340ceee",
   "metadata": {
    "id": "3bf3acc9"
   },
   "source": [
    "We’ll use an OpenAI chat model, SQLChatMessageHistory with Sqlite, embeddings and a Chroma vector store in this walkthrough, but everything shown here works with any ChatModel or LLM, Embeddings, and VectorStore or Retriever.\n",
    "\n",
    "We’ll use the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45eda5e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-02T03:55:32.589Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28445,
     "status": "ok",
     "timestamp": 1709351008927,
     "user": {
      "displayName": "Picker",
      "userId": "08933220343927086323"
     },
     "user_tz": -480
    },
    "id": "133f8484",
    "outputId": "8773fb72-bffb-4871-d8a3-527615f2a83a"
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet  langchain langchain-community langchainhub langchain-openai chromadb bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8e56e1",
   "metadata": {
    "id": "63fb6a78"
   },
   "source": [
    "We need to set environment variable OPENAI_API_KEY, which can be done directly or loaded from a .env file like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b04e02",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-02T03:20:53.509Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12323,
     "status": "ok",
     "timestamp": 1709351023982,
     "user": {
      "displayName": "Picker",
      "userId": "08933220343927086323"
     },
     "user_tz": -480
    },
    "id": "1d41841f",
    "outputId": "2856131a-8ec9-433b-d9a6-3fea45d03211"
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
    "\n",
    "# import dotenv\n",
    "# dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7358b586",
   "metadata": {
    "id": "751fcbaa"
   },
   "source": [
    "### LangSmith\n",
    "\n",
    "Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.\n",
    "\n",
    "Note that LangSmith is not needed, but it is helpful. If you do want to use LangSmith, after you sign up at the link above, make sure to set your environment variables to start logging traces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151dd2bc",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-02T03:21:55.635Z"
    },
    "id": "119024ea"
   },
   "outputs": [],
   "source": [
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325be73e",
   "metadata": {
    "id": "855d4a67"
   },
   "source": [
    "## Chat With MessageHisotry Using RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67762f92",
   "metadata": {
    "id": "b735273e"
   },
   "source": [
    "Load the file from [lilianweng's blog](https://lilianweng.github.io/posts/2023-06-23-agent/), do chunk and index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba1c3b8",
   "metadata": {
    "executionInfo": {
     "elapsed": 3036,
     "status": "ok",
     "timestamp": 1709351109606,
     "user": {
      "displayName": "Picker",
      "userId": "08933220343927086323"
     },
     "user_tz": -480
    },
    "id": "409fc990"
   },
   "outputs": [],
   "source": [
    "import bs4\n",
    "\n",
    "from operator import itemgetter\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_community.chat_message_histories import SQLChatMessageHistory\n",
    "\n",
    "# Load, chunk and index the contents of the blog.\n",
    "bs_strainer = bs4.SoupStrainer(class_=(\"post-content\", \"post-title\", \"post-header\"))\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs={\"parse_only\": bs_strainer},\n",
    ")\n",
    "docs = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    \"\"\"\n",
    "    Format a list of document objects.\n",
    "\n",
    "    Args:\n",
    "    - docs (list): A list containing document objects.\n",
    "\n",
    "    Returns:\n",
    "    - str: The formatted string where the page_content attribute of each document object is concatenated, separated by two newline characters.\n",
    "    \"\"\"\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baf2632",
   "metadata": {
    "id": "9b5d99c8"
   },
   "source": [
    "create prompt and add model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e2e261",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-02T03:34:32.822Z"
    },
    "executionInfo": {
     "elapsed": 318,
     "status": "ok",
     "timestamp": 1709351116856,
     "user": {
      "displayName": "Picker",
      "userId": "08933220343927086323"
     },
     "user_tz": -480
    },
    "id": "41ff396b"
   },
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You're an assistant who's good at something. Here is some {context}\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1dc5ed",
   "metadata": {
    "id": "4ba47d1c"
   },
   "source": [
    "create the chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fa7981",
   "metadata": {
    "executionInfo": {
     "elapsed": 330,
     "status": "ok",
     "timestamp": 1709351130129,
     "user": {
      "displayName": "Picker",
      "userId": "08933220343927086323"
     },
     "user_tz": -480
    },
    "id": "6f746717"
   },
   "outputs": [],
   "source": [
    "context = itemgetter(\"question\") | retriever | format_docs\n",
    "first_step = RunnablePassthrough.assign(context=context)\n",
    "chain = first_step | prompt | llm\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    lambda session_id: SQLChatMessageHistory(\n",
    "        session_id=session_id, connection_string=\"sqlite:///sqlite.db\"\n",
    "    ),\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1706e9",
   "metadata": {
    "executionInfo": {
     "elapsed": 314,
     "status": "ok",
     "timestamp": 1709351138210,
     "user": {
      "displayName": "Picker",
      "userId": "08933220343927086323"
     },
     "user_tz": -480
    },
    "id": "3fdbcf1d"
   },
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"test_session\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99457e2e",
   "metadata": {
    "id": "9964a160"
   },
   "source": [
    "try the first question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb21dffe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2943,
     "status": "ok",
     "timestamp": 1709351145243,
     "user": {
      "displayName": "Picker",
      "userId": "08933220343927086323"
     },
     "user_tz": -480
    },
    "id": "83cfb629",
    "outputId": "aeed2858-48ac-4a85-d6b2-2cd8b7c7551e"
   },
   "outputs": [],
   "source": [
    "result = chain_with_history.invoke(\n",
    "    {\"question\": \"What does the document mainly contain?\"}, config\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85014f2b",
   "metadata": {
    "id": "cec9ba00"
   },
   "source": [
    "try the section question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44df3ec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1129,
     "status": "ok",
     "timestamp": 1709351177625,
     "user": {
      "displayName": "Picker",
      "userId": "08933220343927086323"
     },
     "user_tz": -480
    },
    "id": "01342e8a",
    "outputId": "4e9be6a9-88da-4381-c0a7-08114c6ab1c9"
   },
   "outputs": [],
   "source": [
    "result = chain_with_history.invoke({\"question\": \"Summarize a title for me\"}, config)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
